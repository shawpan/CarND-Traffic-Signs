{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Classification with Keras\n",
    "\n",
    "Keras exists to make coding deep neural networks simpler. To demonstrate just how easy it is, you’re going to use Keras to build a convolutional neural network in a few dozen lines of code.\n",
    "\n",
    "You’ll be connecting the concepts from the previous lessons to the methods that Keras provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The network you'll build with Keras is similar to the example that you can find in Keras’s GitHub repository that builds out a [convolutional neural network for MNIST](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py). \n",
    "\n",
    "However, instead of using the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, you're going to use the [German Traffic Sign Recognition Benchmark](http://benchmark.ini.rub.de/?section=gtsrb&subsection=news) dataset that you've used previously.\n",
    "\n",
    "You can download pickle files with sanitized traffic sign data here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Here are the steps you'll take to build the network:\n",
    "\n",
    "1. First load the training data and do a train/validation split.\n",
    "2. Preprocess data.\n",
    "3. Build a feedforward neural network to classify traffic signs.\n",
    "4. Build a convolutional neural network to classify traffic signs.\n",
    "5. Evaluate performance of final neural network on testing data.\n",
    "\n",
    "Keep an eye on the network’s accuracy over time. Once the accuracy reaches the 98% range, you can be confident that you’ve built and trained an effective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Start by importing the data from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Implement load the data here.\n",
    "with open('traffic-signs-data/train.p', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Network\n",
    "Split the training data into a training and validation set.\n",
    "\n",
    "Measure the [validation accuracy](https://keras.io/models/sequential/) of the network after two training epochs.\n",
    "\n",
    "Hint: [Use the `train_test_split()` method](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use `train_test_split` here.\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['features'], data['labels'], random_state=0, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "assert(X_train.shape[0] == y_train.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "assert(X_train.shape[1:] == (32,32,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "assert(X_val.shape[0] == y_val.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "assert(X_val.shape[1:] == (32,32,3)), \"The dimensions of the images are not 32 x 32 x 3.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "\n",
    "Now that you've loaded the training data, preprocess the data such that it's in the range between -0.5 and 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Implement data normalization here.\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_train = X_train / 255 - 0.5\n",
    "X_val = X_val / 255 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "assert(math.isclose(np.min(X_train), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_train), 0.5, abs_tol=1e-5)), \"The range of the training data is: %.1f to %.1f\" % (np.min(X_train), np.max(X_train))\n",
    "assert(math.isclose(np.min(X_val), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_val), 0.5, abs_tol=1e-5)), \"The range of the validation data is: %.1f to %.1f\" % (np.min(X_val), np.max(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Two-Layer Feedfoward Network\n",
    "\n",
    "The code you've written so far is for data processing, not specific to Keras. Here you're going to build Keras-specific code.\n",
    "\n",
    "Build a two-layer feedforward neural network, with 128 neurons in the fully-connected hidden layer. \n",
    "\n",
    "To get started, review the Keras documentation about [models](https://keras.io/models/sequential/) and [layers](https://keras.io/layers/core/).\n",
    "\n",
    "The Keras example of a [Multi-Layer Perceptron](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py) network is similar to what you need to do here. Use that as a guide, but keep in mind that there are a number of differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build a two-layer feedforward neural network with Keras here.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(32*32*3,)))\n",
    "model.add(Dense(43, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "dense_layers = []\n",
    "for l in model.layers:\n",
    "    if type(l) == Dense:\n",
    "        dense_layers.append(l)\n",
    "assert(len(dense_layers) == 2), \"There should be 2 Dense layers.\"\n",
    "d1 = dense_layers[0]\n",
    "d2 = dense_layers[1]\n",
    "assert(d1.input_shape == (None, 3072))\n",
    "assert(d1.output_shape == (None, 128))\n",
    "assert(d2.input_shape == (None, 128))\n",
    "assert(d2.output_shape == (None, 43))\n",
    "\n",
    "last_layer = model.layers[-1]\n",
    "assert(last_layer.activation.__name__ == 'softmax'), \"Last layer should be softmax activation, is {}.\".format(last_layer.activation.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_1 (None, 3072) (None, 128) <function relu at 0x123da3e18>\n",
      "dense_2 (None, 128) (None, 43) <function softmax at 0x123da3c80>\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "for l in model.layers:\n",
    "    print(l.name, l.input_shape, l.output_shape, l.activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "Compile and train the network for 2 epochs. [Use the `adam` optimizer, with `categorical_crossentropy` loss.](https://keras.io/models/sequential/)\n",
    "\n",
    "Hint 1: In order to use categorical cross entropy, you will need to [one-hot encode the labels](https://github.com/fchollet/keras/blob/master/keras/utils/np_utils.py).\n",
    "\n",
    "Hint 2: In order to pass the input images to the fully-connected hidden layer, you will need to [reshape the input](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py).\n",
    "\n",
    "Hint 3: Keras's `.fit()` method returns a `History.history` object, which the tests below use. Save that to a variable named `history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 128)           393344      dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 43)            5547        dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 398891\n",
      "____________________________________________________________________________________________________\n",
      "Train on 26270 samples, validate on 12939 samples\n",
      "Epoch 1/20\n",
      "26270/26270 [==============================] - 5s - loss: 1.9344 - acc: 0.5038 - val_loss: 1.2412 - val_acc: 0.6443\n",
      "Epoch 2/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.9499 - acc: 0.7560 - val_loss: 0.8414 - val_acc: 0.7645\n",
      "Epoch 3/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.6743 - acc: 0.8275 - val_loss: 0.6165 - val_acc: 0.8221\n",
      "Epoch 4/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.5238 - acc: 0.8673 - val_loss: 0.5417 - val_acc: 0.8578\n",
      "Epoch 5/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.4425 - acc: 0.8887 - val_loss: 0.4360 - val_acc: 0.8815\n",
      "Epoch 6/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.3895 - acc: 0.9011 - val_loss: 0.4209 - val_acc: 0.8779\n",
      "Epoch 7/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.3446 - acc: 0.9109 - val_loss: 0.4038 - val_acc: 0.8954\n",
      "Epoch 8/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.3196 - acc: 0.9179 - val_loss: 0.3445 - val_acc: 0.9111\n",
      "Epoch 9/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.2871 - acc: 0.9268 - val_loss: 0.3199 - val_acc: 0.9095\n",
      "Epoch 10/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.2725 - acc: 0.9281 - val_loss: 0.3117 - val_acc: 0.9160\n",
      "Epoch 11/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.2549 - acc: 0.9332 - val_loss: 0.4140 - val_acc: 0.8824\n",
      "Epoch 12/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.2476 - acc: 0.9322 - val_loss: 0.3943 - val_acc: 0.8746\n",
      "Epoch 13/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.2215 - acc: 0.9431 - val_loss: 0.2566 - val_acc: 0.9386\n",
      "Epoch 14/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.2039 - acc: 0.9474 - val_loss: 0.2595 - val_acc: 0.9285\n",
      "Epoch 15/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.1892 - acc: 0.9504 - val_loss: 0.3369 - val_acc: 0.9076\n",
      "Epoch 16/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.1905 - acc: 0.9509 - val_loss: 0.2603 - val_acc: 0.9308\n",
      "Epoch 17/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.2166 - acc: 0.9402 - val_loss: 0.3131 - val_acc: 0.9056\n",
      "Epoch 18/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.1813 - acc: 0.9495 - val_loss: 0.2720 - val_acc: 0.9188\n",
      "Epoch 19/20\n",
      "26270/26270 [==============================] - 5s - loss: 0.1604 - acc: 0.9569 - val_loss: 0.2632 - val_acc: 0.9301\n",
      "Epoch 20/20\n",
      "26270/26270 [==============================] - 6s - loss: 0.1593 - acc: 0.9570 - val_loss: 0.2459 - val_acc: 0.9266\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compile and train the model here.\n",
    "from keras.utils import np_utils\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 43)\n",
    "Y_val = np_utils.to_categorical(y_val, 43)\n",
    "\n",
    "X_train_flat = X_train.reshape(-1, 32*32*3)\n",
    "X_val_flat = X_val.reshape(-1, 32*32*3)\n",
    "\n",
    "model.summary()\n",
    "# TODO: Compile and train the model here.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train_flat, Y_train,\n",
    "                    batch_size=128, nb_epoch=20,\n",
    "                    verbose=1, validation_data=(X_val_flat, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "assert(history.history['acc'][-1] > 0.92), \"The training accuracy was: %.3f\" % history.history['acc'][-1]\n",
    "assert(history.history['val_acc'][-1] > 0.9), \"The validation accuracy is: %.3f\" % history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Accuracy**: 92.66 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations\n",
    "You've built a feedforward neural network in Keras!\n",
    "\n",
    "Don't stop here! Next, you'll add a convolutional layer to drive.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "Build a new network, similar to your existing network. Before the hidden layer, add a 3x3 [convolutional layer](https://keras.io/layers/convolutional/#convolution2d) with 32 filters and valid padding.\n",
    "\n",
    "Then compile and train the network.\n",
    "\n",
    "Hint 1: The Keras example of a [convolutional neural network](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) for MNIST would be a good example to review.\n",
    "\n",
    "Hint 2: Now that the first layer of the network is a convolutional layer, you no longer need to reshape the input images before passing them to the network. You might need to reload your training data to recover the original shape.\n",
    "\n",
    "Hint 3: Add a [`Flatten()` layer](https://keras.io/layers/core/#flatten) between the convolutional layer and the fully-connected hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 30, 30, 32)    896         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 28800)         0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           3686528     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 43)            5547        dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3692971\n",
      "____________________________________________________________________________________________________\n",
      "Train on 26270 samples, validate on 12939 samples\n",
      "Epoch 1/20\n",
      "26270/26270 [==============================] - 63s - loss: 1.2381 - acc: 0.6804 - val_loss: 0.4767 - val_acc: 0.8657\n",
      "Epoch 2/20\n",
      "26270/26270 [==============================] - 64s - loss: 0.3179 - acc: 0.9226 - val_loss: 0.2439 - val_acc: 0.9399\n",
      "Epoch 3/20\n",
      "26270/26270 [==============================] - 57s - loss: 0.1665 - acc: 0.9616 - val_loss: 0.1793 - val_acc: 0.9576\n",
      "Epoch 4/20\n",
      "26270/26270 [==============================] - 57s - loss: 0.1061 - acc: 0.9761 - val_loss: 0.1737 - val_acc: 0.9553\n",
      "Epoch 5/20\n",
      "26270/26270 [==============================] - 58s - loss: 0.0679 - acc: 0.9858 - val_loss: 0.1569 - val_acc: 0.9625\n",
      "Epoch 6/20\n",
      "26270/26270 [==============================] - 58s - loss: 0.0591 - acc: 0.9861 - val_loss: 0.1368 - val_acc: 0.9677\n",
      "Epoch 7/20\n",
      "26270/26270 [==============================] - 61s - loss: 0.0455 - acc: 0.9904 - val_loss: 0.1462 - val_acc: 0.9665\n",
      "Epoch 8/20\n",
      "26270/26270 [==============================] - 59s - loss: 0.0491 - acc: 0.9880 - val_loss: 0.1576 - val_acc: 0.9641\n",
      "Epoch 9/20\n",
      "26270/26270 [==============================] - 70s - loss: 0.0329 - acc: 0.9932 - val_loss: 0.1239 - val_acc: 0.9729\n",
      "Epoch 10/20\n",
      "26270/26270 [==============================] - 65s - loss: 0.0308 - acc: 0.9933 - val_loss: 0.1901 - val_acc: 0.9573\n",
      "Epoch 11/20\n",
      "26270/26270 [==============================] - 59s - loss: 0.0282 - acc: 0.9929 - val_loss: 0.1365 - val_acc: 0.9692\n",
      "Epoch 12/20\n",
      "26270/26270 [==============================] - 58s - loss: 0.0172 - acc: 0.9967 - val_loss: 0.1419 - val_acc: 0.9662\n",
      "Epoch 13/20\n",
      "26270/26270 [==============================] - 60s - loss: 0.0196 - acc: 0.9956 - val_loss: 0.1343 - val_acc: 0.9723\n",
      "Epoch 14/20\n",
      "26270/26270 [==============================] - 58s - loss: 0.0115 - acc: 0.9981 - val_loss: 0.1181 - val_acc: 0.9767\n",
      "Epoch 15/20\n",
      "26270/26270 [==============================] - 80s - loss: 0.0072 - acc: 0.9994 - val_loss: 0.1166 - val_acc: 0.9778\n",
      "Epoch 16/20\n",
      "26270/26270 [==============================] - 73s - loss: 0.0214 - acc: 0.9951 - val_loss: 0.4126 - val_acc: 0.9336\n",
      "Epoch 17/20\n",
      "26270/26270 [==============================] - 74s - loss: 0.0643 - acc: 0.9830 - val_loss: 0.2068 - val_acc: 0.9580\n",
      "Epoch 18/20\n",
      "26270/26270 [==============================] - 68s - loss: 0.0296 - acc: 0.9923 - val_loss: 0.1630 - val_acc: 0.9691\n",
      "Epoch 19/20\n",
      "26270/26270 [==============================] - 66s - loss: 0.0218 - acc: 0.9952 - val_loss: 0.1365 - val_acc: 0.9728\n",
      "Epoch 20/20\n",
      "26270/26270 [==============================] - 70s - loss: 0.0084 - acc: 0.9983 - val_loss: 0.1236 - val_acc: 0.9780\n"
     ]
    }
   ],
   "source": [
    "# TODO: Re-construct the network and add a convolutional layer before the first fully-connected layer.\n",
    "# NOTE: RELOAD DATA & NORMALIZE BEFORE RUNNING \n",
    "from keras.layers import Conv2D, Flatten\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 43)\n",
    "Y_val = np_utils.to_categorical(y_val, 43)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, 3, input_shape=(32, 32, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "# TODO: Compile and train the model here.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=128, nb_epoch=20,\n",
    "                    verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "assert(history.history['val_acc'][-1] > 0.93), \"The validation accuracy is: %.3f\" % history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Accuracy**: 97.8 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "Re-construct your network and add a 2x2 [pooling layer](https://keras.io/layers/pooling/#maxpooling2d) immediately following your convolutional layer.\n",
    "\n",
    "Then compile and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_2 (Convolution2D)  (None, 30, 30, 32)    896         convolution2d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 15, 15, 32)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 15, 15, 32)    0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 7200)          0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 128)           921728      flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 43)            5547        dense_5[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 928171\n",
      "____________________________________________________________________________________________________\n",
      "Train on 26270 samples, validate on 12939 samples\n",
      "Epoch 1/20\n",
      "26270/26270 [==============================] - 42s - loss: 1.7652 - acc: 0.5351 - val_loss: 0.8327 - val_acc: 0.7760\n",
      "Epoch 2/20\n",
      "26270/26270 [==============================] - 44s - loss: 0.5627 - acc: 0.8565 - val_loss: 0.4262 - val_acc: 0.8851\n",
      "Epoch 3/20\n",
      "26270/26270 [==============================] - 43s - loss: 0.3055 - acc: 0.9290 - val_loss: 0.2725 - val_acc: 0.9341\n",
      "Epoch 4/20\n",
      "26270/26270 [==============================] - 43s - loss: 0.2050 - acc: 0.9532 - val_loss: 0.2092 - val_acc: 0.9548\n",
      "Epoch 5/20\n",
      "26270/26270 [==============================] - 39s - loss: 0.1495 - acc: 0.9674 - val_loss: 0.1801 - val_acc: 0.9575\n",
      "Epoch 6/20\n",
      "26270/26270 [==============================] - 42s - loss: 0.1138 - acc: 0.9749 - val_loss: 0.1654 - val_acc: 0.9607\n",
      "Epoch 7/20\n",
      "26270/26270 [==============================] - 38s - loss: 0.0860 - acc: 0.9827 - val_loss: 0.1485 - val_acc: 0.9688\n",
      "Epoch 8/20\n",
      "26270/26270 [==============================] - 39s - loss: 0.0744 - acc: 0.9839 - val_loss: 0.1475 - val_acc: 0.9664\n",
      "Epoch 9/20\n",
      "26270/26270 [==============================] - 36s - loss: 0.0598 - acc: 0.9876 - val_loss: 0.1308 - val_acc: 0.9716\n",
      "Epoch 10/20\n",
      "26270/26270 [==============================] - 39s - loss: 0.0494 - acc: 0.9895 - val_loss: 0.1242 - val_acc: 0.9743\n",
      "Epoch 11/20\n",
      "26270/26270 [==============================] - 49s - loss: 0.0424 - acc: 0.9914 - val_loss: 0.1358 - val_acc: 0.9700\n",
      "Epoch 12/20\n",
      "26270/26270 [==============================] - 37s - loss: 0.0400 - acc: 0.9920 - val_loss: 0.1897 - val_acc: 0.9619\n",
      "Epoch 13/20\n",
      "26270/26270 [==============================] - 40s - loss: 0.0323 - acc: 0.9936 - val_loss: 0.1150 - val_acc: 0.9757\n",
      "Epoch 14/20\n",
      "26270/26270 [==============================] - 38s - loss: 0.0220 - acc: 0.9965 - val_loss: 0.1187 - val_acc: 0.9757\n",
      "Epoch 15/20\n",
      "26270/26270 [==============================] - 49s - loss: 0.0337 - acc: 0.9930 - val_loss: 0.1551 - val_acc: 0.9647\n",
      "Epoch 16/20\n",
      "26270/26270 [==============================] - 43s - loss: 0.0372 - acc: 0.9911 - val_loss: 0.1406 - val_acc: 0.9716\n",
      "Epoch 17/20\n",
      "26270/26270 [==============================] - 43s - loss: 0.0294 - acc: 0.9929 - val_loss: 0.1279 - val_acc: 0.9729\n",
      "Epoch 18/20\n",
      "26270/26270 [==============================] - 37s - loss: 0.0160 - acc: 0.9976 - val_loss: 0.1107 - val_acc: 0.9786\n",
      "Epoch 19/20\n",
      "26270/26270 [==============================] - 37s - loss: 0.0116 - acc: 0.9986 - val_loss: 0.1090 - val_acc: 0.9798\n",
      "Epoch 20/20\n",
      "26270/26270 [==============================] - 37s - loss: 0.0107 - acc: 0.9987 - val_loss: 0.1131 - val_acc: 0.9787\n"
     ]
    }
   ],
   "source": [
    "# TODO: Re-construct the network and add a pooling layer after the convolutional layer.\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, Activation\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 43)\n",
    "Y_val = np_utils.to_categorical(y_val, 43)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, 3, input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "# TODO: Compile and train the model here.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=128, nb_epoch=20,\n",
    "                    verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "assert(history.history['val_acc'][-1] > 0.93), \"The validation accuracy is: %.3f\" % history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Accuracy**: 97.87 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "Re-construct your network and add [dropout](https://keras.io/layers/core/#dropout) after the pooling layer. Set the dropout rate to 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_6 (Convolution2D)  (None, 30, 30, 32)    896         convolution2d_input_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 15, 15, 32)    0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 15, 15, 32)    0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 15, 15, 32)    0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 7200)          0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 128)           921728      flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 43)            5547        dense_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 928171\n",
      "____________________________________________________________________________________________________\n",
      "Train on 26270 samples, validate on 12939 samples\n",
      "Epoch 1/20\n",
      "26270/26270 [==============================] - 39s - loss: 1.7433 - acc: 0.5398 - val_loss: 0.7683 - val_acc: 0.8008\n",
      "Epoch 2/20\n",
      "26270/26270 [==============================] - 39s - loss: 0.5819 - acc: 0.8469 - val_loss: 0.3922 - val_acc: 0.8995\n",
      "Epoch 3/20\n",
      "26270/26270 [==============================] - 39s - loss: 0.3545 - acc: 0.9081 - val_loss: 0.2587 - val_acc: 0.9440\n",
      "Epoch 4/20\n",
      "26270/26270 [==============================] - 39s - loss: 0.2574 - acc: 0.9336 - val_loss: 0.2100 - val_acc: 0.9510\n",
      "Epoch 5/20\n",
      "26270/26270 [==============================] - 39s - loss: 0.2088 - acc: 0.9466 - val_loss: 0.1849 - val_acc: 0.9563\n",
      "Epoch 6/20\n",
      "26270/26270 [==============================] - 39s - loss: 0.1772 - acc: 0.9539 - val_loss: 0.1929 - val_acc: 0.9544\n",
      "Epoch 7/20\n",
      "26270/26270 [==============================] - 39s - loss: 0.1501 - acc: 0.9609 - val_loss: 0.1542 - val_acc: 0.9622\n",
      "Epoch 8/20\n",
      "26270/26270 [==============================] - 39s - loss: 0.1379 - acc: 0.9627 - val_loss: 0.1400 - val_acc: 0.9657\n",
      "Epoch 9/20\n",
      "26270/26270 [==============================] - 45s - loss: 0.1220 - acc: 0.9671 - val_loss: 0.1300 - val_acc: 0.9692\n",
      "Epoch 10/20\n",
      "26270/26270 [==============================] - 42s - loss: 0.1131 - acc: 0.9685 - val_loss: 0.1241 - val_acc: 0.9723\n",
      "Epoch 11/20\n",
      "26270/26270 [==============================] - 43s - loss: 0.1061 - acc: 0.9703 - val_loss: 0.1125 - val_acc: 0.9754\n",
      "Epoch 12/20\n",
      "26270/26270 [==============================] - 43s - loss: 0.0959 - acc: 0.9723 - val_loss: 0.1104 - val_acc: 0.9756\n",
      "Epoch 13/20\n",
      "26270/26270 [==============================] - 45s - loss: 0.0899 - acc: 0.9747 - val_loss: 0.1151 - val_acc: 0.9749\n",
      "Epoch 14/20\n",
      "26270/26270 [==============================] - 42s - loss: 0.0832 - acc: 0.9778 - val_loss: 0.0982 - val_acc: 0.9805\n",
      "Epoch 15/20\n",
      "26270/26270 [==============================] - 40s - loss: 0.0722 - acc: 0.9810 - val_loss: 0.0963 - val_acc: 0.9800\n",
      "Epoch 16/20\n",
      "26270/26270 [==============================] - 40s - loss: 0.0771 - acc: 0.9791 - val_loss: 0.1030 - val_acc: 0.9777\n",
      "Epoch 17/20\n",
      "26270/26270 [==============================] - 40s - loss: 0.0724 - acc: 0.9786 - val_loss: 0.1016 - val_acc: 0.9754\n",
      "Epoch 18/20\n",
      "26270/26270 [==============================] - 52s - loss: 0.0642 - acc: 0.9826 - val_loss: 0.0869 - val_acc: 0.9826\n",
      "Epoch 19/20\n",
      "26270/26270 [==============================] - 43s - loss: 0.0558 - acc: 0.9846 - val_loss: 0.0917 - val_acc: 0.9818\n",
      "Epoch 20/20\n",
      "26270/26270 [==============================] - 40s - loss: 0.0679 - acc: 0.9805 - val_loss: 0.0991 - val_acc: 0.9801\n"
     ]
    }
   ],
   "source": [
    "# TODO: Re-construct the network and add dropout after the pooling layer.\n",
    "from keras.layers import Dropout\n",
    "import tensorflow as tf\n",
    "# Fix of AttributeError: module 'tensorflow.python' has no attribute 'control_flow_ops'\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, 3, input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add((Dropout(0.5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "# TODO: Compile and train the model here.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=128, nb_epoch=20,\n",
    "                    verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "assert(history.history['val_acc'][-1] > 0.93), \"The validation accuracy is: %.3f\" % history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Validation Accuracy**: 98.01 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Congratulations! You've built a neural network with convolutions, pooling, dropout, and fully-connected layers, all in just a few lines of code.\n",
    "\n",
    "Have fun with the model and see how well you can do! Add more layers, or regularization, or different padding, or batches, or more training epochs.\n",
    "\n",
    "What is the best validation accuracy you can achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_11 (Convolution2D) (None, 28, 28, 6)     456         convolution2d_input_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 14, 14, 6)     0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 14, 14, 6)     0           maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 10, 10, 16)    2416        activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_11 (MaxPooling2D)   (None, 5, 5, 16)      0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 5, 5, 16)      0           maxpooling2d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 400)           0           activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 128)           51328       flatten_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 128)           0           dense_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 128)           0           dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 43)            5547        activation_12[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 59747\n",
      "____________________________________________________________________________________________________\n",
      "Train on 26270 samples, validate on 12939 samples\n",
      "Epoch 1/100\n",
      "26270/26270 [==============================] - 37s - loss: 2.5648 - acc: 0.3171 - val_loss: 1.1828 - val_acc: 0.6949\n",
      "Epoch 2/100\n",
      "26270/26270 [==============================] - 40s - loss: 1.0684 - acc: 0.6810 - val_loss: 0.5342 - val_acc: 0.8708\n",
      "Epoch 3/100\n",
      "26270/26270 [==============================] - 44s - loss: 0.6546 - acc: 0.8070 - val_loss: 0.3421 - val_acc: 0.9170\n",
      "Epoch 4/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.4664 - acc: 0.8613 - val_loss: 0.2535 - val_acc: 0.9365\n",
      "Epoch 5/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.3630 - acc: 0.8920 - val_loss: 0.1905 - val_acc: 0.9567\n",
      "Epoch 6/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.2958 - acc: 0.9108 - val_loss: 0.1670 - val_acc: 0.9607\n",
      "Epoch 7/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.2464 - acc: 0.9273 - val_loss: 0.1318 - val_acc: 0.9682\n",
      "Epoch 8/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.2067 - acc: 0.9381 - val_loss: 0.1190 - val_acc: 0.9726\n",
      "Epoch 9/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.1929 - acc: 0.9436 - val_loss: 0.1004 - val_acc: 0.9753\n",
      "Epoch 10/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.1674 - acc: 0.9504 - val_loss: 0.0914 - val_acc: 0.9804\n",
      "Epoch 11/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.1482 - acc: 0.9546 - val_loss: 0.0867 - val_acc: 0.9791\n",
      "Epoch 12/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.1379 - acc: 0.9574 - val_loss: 0.0792 - val_acc: 0.9820\n",
      "Epoch 13/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.1293 - acc: 0.9611 - val_loss: 0.0699 - val_acc: 0.9846\n",
      "Epoch 14/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.1177 - acc: 0.9644 - val_loss: 0.0708 - val_acc: 0.9845\n",
      "Epoch 15/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.1063 - acc: 0.9669 - val_loss: 0.0698 - val_acc: 0.9838\n",
      "Epoch 16/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.1005 - acc: 0.9695 - val_loss: 0.0625 - val_acc: 0.9862\n",
      "Epoch 17/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0936 - acc: 0.9722 - val_loss: 0.0598 - val_acc: 0.9861\n",
      "Epoch 18/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0851 - acc: 0.9737 - val_loss: 0.0583 - val_acc: 0.9872\n",
      "Epoch 19/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0810 - acc: 0.9751 - val_loss: 0.0554 - val_acc: 0.9875\n",
      "Epoch 20/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0739 - acc: 0.9774 - val_loss: 0.0549 - val_acc: 0.9876\n",
      "Epoch 21/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0797 - acc: 0.9759 - val_loss: 0.0532 - val_acc: 0.9882\n",
      "Epoch 22/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0682 - acc: 0.9789 - val_loss: 0.0536 - val_acc: 0.9884\n",
      "Epoch 23/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0644 - acc: 0.9816 - val_loss: 0.0527 - val_acc: 0.9882\n",
      "Epoch 24/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0647 - acc: 0.9791 - val_loss: 0.0499 - val_acc: 0.9892\n",
      "Epoch 25/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0613 - acc: 0.9816 - val_loss: 0.0457 - val_acc: 0.9893\n",
      "Epoch 26/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0559 - acc: 0.9826 - val_loss: 0.0479 - val_acc: 0.9898\n",
      "Epoch 27/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0572 - acc: 0.9823 - val_loss: 0.0468 - val_acc: 0.9900\n",
      "Epoch 28/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0546 - acc: 0.9831 - val_loss: 0.0490 - val_acc: 0.9893\n",
      "Epoch 29/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0484 - acc: 0.9850 - val_loss: 0.0474 - val_acc: 0.9893\n",
      "Epoch 30/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0493 - acc: 0.9849 - val_loss: 0.0447 - val_acc: 0.9895\n",
      "Epoch 31/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0489 - acc: 0.9848 - val_loss: 0.0453 - val_acc: 0.9906\n",
      "Epoch 32/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0432 - acc: 0.9866 - val_loss: 0.0507 - val_acc: 0.9883\n",
      "Epoch 33/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0502 - acc: 0.9846 - val_loss: 0.0446 - val_acc: 0.9903\n",
      "Epoch 34/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0475 - acc: 0.9851 - val_loss: 0.0446 - val_acc: 0.9911\n",
      "Epoch 35/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0460 - acc: 0.9850 - val_loss: 0.0471 - val_acc: 0.9898\n",
      "Epoch 36/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0420 - acc: 0.9862 - val_loss: 0.0441 - val_acc: 0.9908\n",
      "Epoch 37/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0409 - acc: 0.9868 - val_loss: 0.0452 - val_acc: 0.9903\n",
      "Epoch 38/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0377 - acc: 0.9876 - val_loss: 0.0471 - val_acc: 0.9904\n",
      "Epoch 39/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0380 - acc: 0.9878 - val_loss: 0.0440 - val_acc: 0.9913\n",
      "Epoch 40/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0378 - acc: 0.9875 - val_loss: 0.0415 - val_acc: 0.9915\n",
      "Epoch 41/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0379 - acc: 0.9880 - val_loss: 0.0415 - val_acc: 0.9923\n",
      "Epoch 42/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0377 - acc: 0.9882 - val_loss: 0.0438 - val_acc: 0.9911\n",
      "Epoch 43/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0340 - acc: 0.9893 - val_loss: 0.0449 - val_acc: 0.9905\n",
      "Epoch 44/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0359 - acc: 0.9883 - val_loss: 0.0421 - val_acc: 0.9906\n",
      "Epoch 45/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0337 - acc: 0.9888 - val_loss: 0.0457 - val_acc: 0.9914\n",
      "Epoch 46/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0314 - acc: 0.9898 - val_loss: 0.0438 - val_acc: 0.9912\n",
      "Epoch 47/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0307 - acc: 0.9894 - val_loss: 0.0471 - val_acc: 0.9915\n",
      "Epoch 48/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0277 - acc: 0.9916 - val_loss: 0.0423 - val_acc: 0.9916\n",
      "Epoch 49/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0328 - acc: 0.9892 - val_loss: 0.0427 - val_acc: 0.9910\n",
      "Epoch 50/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0337 - acc: 0.9886 - val_loss: 0.0432 - val_acc: 0.9914\n",
      "Epoch 51/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0322 - acc: 0.9895 - val_loss: 0.0464 - val_acc: 0.9901\n",
      "Epoch 52/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0308 - acc: 0.9903 - val_loss: 0.0516 - val_acc: 0.9888\n",
      "Epoch 53/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0275 - acc: 0.9910 - val_loss: 0.0456 - val_acc: 0.9926\n",
      "Epoch 54/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0297 - acc: 0.9906 - val_loss: 0.0436 - val_acc: 0.9918\n",
      "Epoch 55/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0287 - acc: 0.9910 - val_loss: 0.0410 - val_acc: 0.9925\n",
      "Epoch 56/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0294 - acc: 0.9908 - val_loss: 0.0439 - val_acc: 0.9912\n",
      "Epoch 57/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0265 - acc: 0.9912 - val_loss: 0.0422 - val_acc: 0.9932\n",
      "Epoch 58/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0261 - acc: 0.9914 - val_loss: 0.0409 - val_acc: 0.9924\n",
      "Epoch 59/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0255 - acc: 0.9912 - val_loss: 0.0442 - val_acc: 0.9910\n",
      "Epoch 60/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0257 - acc: 0.9925 - val_loss: 0.0428 - val_acc: 0.9921\n",
      "Epoch 61/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0283 - acc: 0.9904 - val_loss: 0.0408 - val_acc: 0.9927\n",
      "Epoch 62/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0280 - acc: 0.9910 - val_loss: 0.0410 - val_acc: 0.9931\n",
      "Epoch 63/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0250 - acc: 0.9921 - val_loss: 0.0408 - val_acc: 0.9920\n",
      "Epoch 64/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0245 - acc: 0.9919 - val_loss: 0.0416 - val_acc: 0.9917\n",
      "Epoch 65/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0230 - acc: 0.9922 - val_loss: 0.0449 - val_acc: 0.9920\n",
      "Epoch 66/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0302 - acc: 0.9906 - val_loss: 0.0451 - val_acc: 0.9925\n",
      "Epoch 67/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0264 - acc: 0.9922 - val_loss: 0.0468 - val_acc: 0.9911\n",
      "Epoch 68/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0209 - acc: 0.9936 - val_loss: 0.0419 - val_acc: 0.9928\n",
      "Epoch 69/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0263 - acc: 0.9922 - val_loss: 0.0420 - val_acc: 0.9929\n",
      "Epoch 70/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0250 - acc: 0.9920 - val_loss: 0.0463 - val_acc: 0.9925\n",
      "Epoch 71/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0183 - acc: 0.9941 - val_loss: 0.0530 - val_acc: 0.9893\n",
      "Epoch 72/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0264 - acc: 0.9920 - val_loss: 0.0454 - val_acc: 0.9913\n",
      "Epoch 73/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0426 - val_acc: 0.9923\n",
      "Epoch 74/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0208 - acc: 0.9935 - val_loss: 0.0441 - val_acc: 0.9920\n",
      "Epoch 75/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0200 - acc: 0.9940 - val_loss: 0.0418 - val_acc: 0.9920\n",
      "Epoch 76/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0205 - acc: 0.9933 - val_loss: 0.0500 - val_acc: 0.9913\n",
      "Epoch 77/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0218 - acc: 0.9927 - val_loss: 0.0431 - val_acc: 0.9928\n",
      "Epoch 78/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0191 - acc: 0.9940 - val_loss: 0.0424 - val_acc: 0.9922\n",
      "Epoch 79/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0238 - acc: 0.9922 - val_loss: 0.0419 - val_acc: 0.9929\n",
      "Epoch 80/100\n",
      "26270/26270 [==============================] - 30s - loss: 0.0253 - acc: 0.9921 - val_loss: 0.0449 - val_acc: 0.9913\n",
      "Epoch 81/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0192 - acc: 0.9939 - val_loss: 0.0452 - val_acc: 0.9919\n",
      "Epoch 82/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0217 - acc: 0.9931 - val_loss: 0.0414 - val_acc: 0.9930\n",
      "Epoch 83/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0197 - acc: 0.9931 - val_loss: 0.0435 - val_acc: 0.9919\n",
      "Epoch 84/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0414 - val_acc: 0.9926\n",
      "Epoch 85/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0267 - acc: 0.9920 - val_loss: 0.0474 - val_acc: 0.9910\n",
      "Epoch 86/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0203 - acc: 0.9935 - val_loss: 0.0420 - val_acc: 0.9929\n",
      "Epoch 87/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0205 - acc: 0.9939 - val_loss: 0.0400 - val_acc: 0.9930\n",
      "Epoch 88/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0203 - acc: 0.9933 - val_loss: 0.0471 - val_acc: 0.9925\n",
      "Epoch 89/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0189 - acc: 0.9936 - val_loss: 0.0447 - val_acc: 0.9924\n",
      "Epoch 90/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0202 - acc: 0.9935 - val_loss: 0.0416 - val_acc: 0.9930\n",
      "Epoch 91/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0208 - acc: 0.9936 - val_loss: 0.0408 - val_acc: 0.9928\n",
      "Epoch 92/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0208 - acc: 0.9931 - val_loss: 0.0488 - val_acc: 0.9917\n",
      "Epoch 93/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0159 - acc: 0.9956 - val_loss: 0.0384 - val_acc: 0.9925\n",
      "Epoch 94/100\n",
      "26270/26270 [==============================] - 31s - loss: 0.0135 - acc: 0.9956 - val_loss: 0.0441 - val_acc: 0.9922\n",
      "Epoch 95/100\n",
      "26270/26270 [==============================] - 29s - loss: 0.0166 - acc: 0.9942 - val_loss: 0.0428 - val_acc: 0.9923\n",
      "Epoch 96/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0175 - acc: 0.9944 - val_loss: 0.0406 - val_acc: 0.9936\n",
      "Epoch 97/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0224 - acc: 0.9938 - val_loss: 0.0446 - val_acc: 0.9928\n",
      "Epoch 98/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0170 - acc: 0.9939 - val_loss: 0.0420 - val_acc: 0.9936\n",
      "Epoch 99/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0227 - acc: 0.9932 - val_loss: 0.0414 - val_acc: 0.9923\n",
      "Epoch 100/100\n",
      "26270/26270 [==============================] - 28s - loss: 0.0188 - acc: 0.9935 - val_loss: 0.0388 - val_acc: 0.9931\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Dropout\n",
    "import tensorflow as tf\n",
    "# Fix of AttributeError: module 'tensorflow.python' has no attribute 'control_flow_ops'\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "model = Sequential()\n",
    "# Conv 1\n",
    "model.add(Conv2D(6, 5, 5, input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "#model.add((Dropout(0.5)))\n",
    "model.add(Activation('relu'))\n",
    "# Conv 2\n",
    "model.add(Conv2D(16, 5, 5, input_shape=(10, 10, 16)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "#model.add((Dropout(0.5)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add((Dropout(0.5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "# TODO: Compile and train the model here.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=128, nb_epoch=100,\n",
    "                    verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Validation Accuracy:** 99.31 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Once you've picked out your best model, it's time to test it.\n",
    "\n",
    "Load up the test data and use the [`evaluate()` method](https://keras.io/models/model/#evaluate) to see how well it does.\n",
    "\n",
    "Hint 1: The `evaluate()` method should return an array of numbers. Use the `metrics_names()` method to get the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12630/12630 [==============================] - 8s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31990488304570319, 0.95439429933460385]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load test data\n",
    "with open('traffic-signs-data/test.p', mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "# TODO: Preprocess data & one-hot encode the labels\n",
    "X_test = test['features']\n",
    "y_test = test['labels']\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "X_test -= 0.5\n",
    "Y_test = np_utils.to_categorical(y_test, 43)\n",
    "\n",
    "# TODO: Evaluate model on test data\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Accuracy:** 95.44 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Keras is a great tool to use if you want to quickly build a neural network and evaluate performance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
